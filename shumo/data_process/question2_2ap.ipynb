{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fq/txj212zs79b9zfmsgd_pc3d40000gn/T/ipykernel_966/738068523.py:203: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[column].fillna(df[column].mean(), inplace=True)\n",
      "/var/folders/fq/txj212zs79b9zfmsgd_pc3d40000gn/T/ipykernel_966/738068523.py:201: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[column].fillna(df[column].mode()[0], inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes detected: 9\n",
      "Epoch [10/100], Loss: 1.3991\n",
      "Epoch [20/100], Loss: 1.2992\n",
      "Epoch [30/100], Loss: 1.3224\n",
      "Epoch [40/100], Loss: 1.3374\n",
      "Epoch [50/100], Loss: 1.2991\n",
      "Epoch [60/100], Loss: 1.3160\n",
      "Epoch [70/100], Loss: 1.2852\n",
      "Epoch [80/100], Loss: 1.2649\n",
      "Epoch [90/100], Loss: 1.3295\n",
      "Epoch [100/100], Loss: 1.3038\n",
      "Classification Accuracy: 0.6410256410256411\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1_5       0.00      0.00      0.00         1\n",
      "        2_10       0.00      0.00      0.00         2\n",
      "        2_11       0.64      1.00      0.78        25\n",
      "         2_4       0.00      0.00      0.00         5\n",
      "         2_5       0.00      0.00      0.00         2\n",
      "         2_7       0.00      0.00      0.00         1\n",
      "         2_8       0.00      0.00      0.00         1\n",
      "         2_9       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.64        39\n",
      "   macro avg       0.08      0.12      0.10        39\n",
      "weighted avg       0.41      0.64      0.50        39\n",
      "\n",
      "['2_11' '2_11' '2_11' '2_11' '2_11' '2_11' '2_11' '2_11' '2_11' '2_11'\n",
      " '2_11' '2_11' '2_11' '2_11' '2_11' '2_11' '2_11' '2_11' '2_11' '2_11'\n",
      " '2_11' '2_11' '2_11' '2_11' '2_11' '2_11' '2_11' '2_11' '2_11' '2_11'\n",
      " '2_11' '2_11' '2_11' '2_11' '2_11' '2_11' '2_11' '2_11' '2_11']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daylight/miniconda3/envs/tangou111/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/daylight/miniconda3/envs/tangou111/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/daylight/miniconda3/envs/tangou111/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1517: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "import os\n",
    "import numpy as np\n",
    "import ast\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, classification_report\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "######## AE #######\n",
    "class EncoderDecoderClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(EncoderDecoderClassifier, self).__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(input_size, hidden_size), nn.ReLU())\n",
    "        # 输出层使用 Softmax 来进行分类\n",
    "        self.classifier = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 封装的分类器\n",
    "class EncoderDecoderClassifierModel:\n",
    "    def __init__(self, input_size, hidden_size, epochs=100, lr=0.001, batch_size=1):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None  # 模型会在fit中初始化，依赖于y_train中的类别数\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        # 自动确定 output_size\n",
    "        num_classes = len(np.unique(y_train))  # 获取 y_train 中的类别数量\n",
    "        print(f\"Number of classes detected: {num_classes}\")\n",
    "\n",
    "        # 初始化模型\n",
    "        self.model = EncoderDecoderClassifier(self.input_size, self.hidden_size, num_classes)\n",
    "\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.long)  # 确保 y_train 是整型类别索引\n",
    "\n",
    "        # 优化器在这里初始化，因为 model 现在才定义好\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "        # 训练模型\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for i in range(0, len(X_train), self.batch_size):\n",
    "                X_batch = X_train[i : i + self.batch_size]\n",
    "                y_batch = y_train[i : i + self.batch_size]\n",
    "\n",
    "                outputs = self.model(X_batch)\n",
    "                loss = self.criterion(outputs, y_batch)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            # 每10个 epoch 打印损失\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch+1}/{self.epochs}], Loss: {epoch_loss / len(X_train):.4f}\")\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_test)\n",
    "            # 使用 argmax 从 softmax 输出中获得类别索引\n",
    "            y_pred = torch.argmax(outputs, dim=1).numpy()\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "################################\n",
    "\n",
    "\n",
    "######## VAE #########\n",
    "class VAEClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, latent_size, output_size):\n",
    "        super(VAEClassifier, self).__init__()\n",
    "\n",
    "        # Encoder: 输出均值和对数方差\n",
    "        self.encoder = nn.Sequential(nn.Linear(input_size, hidden_size), nn.ReLU())\n",
    "        self.fc_mu = nn.Linear(hidden_size, latent_size)  # 均值\n",
    "        self.fc_logvar = nn.Linear(hidden_size, latent_size)  # 对数方差\n",
    "\n",
    "        # 分类器: 在隐变量上添加分类器\n",
    "        self.classifier = nn.Linear(latent_size, output_size)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std  # 重参数化技巧\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        logits = self.classifier(z)  # 分类\n",
    "        return logits, mu, logvar  # 返回分类结果和VAE的参数\n",
    "\n",
    "\n",
    "# 封装的VAE分类器\n",
    "class VAEClassifierModel:\n",
    "    def __init__(self, input_size, hidden_size, latent_size, epochs=100, lr=0.001, batch_size=1):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.latent_size = latent_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.model = None  # 模型会在fit中初始化，依赖于y_train中的类别数\n",
    "        self.classification_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def vae_loss(self, reconstruction_loss, mu, logvar):\n",
    "        # KL散度损失\n",
    "        kl_divergence = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "        return reconstruction_loss + kl_divergence\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        # 自动确定 output_size\n",
    "        num_classes = len(np.unique(y_train))  # 获取 y_train 中的类别数量\n",
    "        print(f\"Number of classes detected: {num_classes}\")\n",
    "\n",
    "        # 初始化模型\n",
    "        self.model = VAEClassifier(self.input_size, self.hidden_size, self.latent_size, num_classes)\n",
    "\n",
    "        X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "        y_train = torch.tensor(y_train, dtype=torch.long)  # 确保 y_train 是整型类别索引\n",
    "\n",
    "        # 优化器在这里初始化，因为 model 现在才定义好\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "\n",
    "        # 训练模型\n",
    "        self.model.train()\n",
    "        for epoch in range(self.epochs):\n",
    "            epoch_loss = 0.0\n",
    "            for i in range(0, len(X_train), self.batch_size):\n",
    "                X_batch = X_train[i : i + self.batch_size]\n",
    "                y_batch = y_train[i : i + self.batch_size]\n",
    "\n",
    "                # 前向传播\n",
    "                logits, mu, logvar = self.model(X_batch)\n",
    "                classification_loss = self.classification_criterion(logits, y_batch)\n",
    "                loss = self.vae_loss(classification_loss, mu, logvar)\n",
    "\n",
    "                # 反向传播和优化\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            # 每10个 epoch 打印损失\n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f\"Epoch [{epoch + 1}/{self.epochs}], Loss: {epoch_loss / len(X_train):.4f}\")\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits, _, _ = self.model(X_test)\n",
    "            # 使用 argmax 从 softmax 输出中获得类别索引\n",
    "            y_pred = torch.argmax(logits, dim=1).numpy()\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "################################\n",
    "\n",
    "\n",
    "def fill_missing_values(df):\n",
    "    # 填充空值\n",
    "    for column in df.columns:\n",
    "        if df[column].isnull().all():\n",
    "            if df[column].dtype == \"object\":\n",
    "                df[column].fillna(0, inplace=True)\n",
    "            else:\n",
    "                df[column].fillna(0, inplace=True)\n",
    "        elif df[column].dtype == \"object\":\n",
    "            df[column].fillna(df[column].mode()[0], inplace=True)\n",
    "        elif pd.api.types.is_numeric_dtype(df[column]):\n",
    "            df[column].fillna(df[column].mean(), inplace=True)\n",
    "        else:\n",
    "            df[column].fillna(method=\"ffill\", inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_files_by_keywords(directory, keywords):\n",
    "    # 遍历指定文件夹，根据关键字数组提取文件。\n",
    "    matched_files = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if all(keyword in filename for keyword in keywords):\n",
    "            matched_files.append(filename)\n",
    "    return matched_files\n",
    "\n",
    "\n",
    "def process_array_string(array_string):\n",
    "    # 去掉最大值、最小值并计算平均值\n",
    "    # 如果输入是单个值，则直接返回\n",
    "    if isinstance(array_string, (int, float)):\n",
    "        return array_string\n",
    "    # 将字符串转换为列表\n",
    "    values = ast.literal_eval(array_string)\n",
    "    # 如果列表长度大于2，去掉最大值和最小值\n",
    "    if len(values) > 2:\n",
    "        values.remove(max(values))\n",
    "        values.remove(min(values))\n",
    "    mean_value = np.mean(values)\n",
    "    return mean_value\n",
    "\n",
    "\n",
    "##### 修改这里 #####\n",
    "ap_name = \"2ap\"\n",
    "project_root = \"/Users/daylight/Desktop/macos/1Code/Competition/2024ShuMo\"\n",
    "data_dir = project_root + \"/data/results/question1_add_column\"\n",
    "training_data_names = get_files_by_keywords(data_dir, [\"training\", ap_name, \"csv\"])\n",
    "training_data_all_path = project_root + f\"/data/processed/training_data_{ap_name}.csv\"\n",
    "os.makedirs(os.path.dirname(training_data_all_path), exist_ok=True)\n",
    "training_data_names = sorted(training_data_names)\n",
    "\n",
    "#### 读取所有训练数据 #####\n",
    "training_data_all = pd.DataFrame()\n",
    "# 初始化一个变量来追踪全局的最大 test_id\n",
    "current_max_test_id = 0\n",
    "file_split_id = []  # 记录分隔文件的id位置\n",
    "for file in training_data_names:\n",
    "    file_path = os.path.join(data_dir, file)\n",
    "    df = pd.read_csv(file_path)\n",
    "    # 获取当前合并DataFrame中的最大test_id，如果为空则设置为0\n",
    "    if not training_data_all.empty:\n",
    "        max_test_id = training_data_all[\"test_id\"].max()\n",
    "    else:\n",
    "        max_test_id = 0\n",
    "    # 调整新df的test_id，保证test_id连续递增\n",
    "    df[\"test_id\"] = df[\"test_id\"] + max_test_id\n",
    "    # 将当前DataFrame追加到总的training_data_all中\n",
    "    training_data_all = pd.concat([training_data_all, df], ignore_index=True)\n",
    "    file_split_id.append(training_data_all[\"test_id\"].max())\n",
    "\n",
    "columns_class = [\"ap_id\", \"sta_id\"]\n",
    "columns_numerical = [\"eirp\", \"nav\", \"add_change\", \"predict seq_time\"]\n",
    "columns_basic = [\"test_id\", \"nss\", \"mcs\", \"protocol\"] + columns_numerical\n",
    "# protocol_name = [\"tcp\", \"udp\"]\n",
    "\n",
    "### 提取对应的列rssi ###\n",
    "ap_0_sta_0 = [\"ap_from_ap_1_mean_ant_rssi\", \"sta_to_ap_0_mean_ant_rssi\", \"sta_to_ap_1_mean_ant_rssi\", \"sta_from_ap_0_mean_ant_rssi\", \"sta_from_ap_1_mean_ant_rssi\", \"sta_from_sta_1_rssi\"]\n",
    "ap_1_sta_1 = [\"ap_from_ap_0_mean_ant_rssi\", \"sta_to_ap_1_mean_ant_rssi\", \"sta_to_ap_0_mean_ant_rssi\", \"sta_from_ap_1_mean_ant_rssi\", \"sta_from_ap_0_mean_ant_rssi\", \"sta_from_sta_0_rssi\"]\n",
    "training_data_all_ap_0 = training_data_all.loc[training_data_all[\"ap_id\"] == \"ap_0\"].copy()\n",
    "for i, column in enumerate(ap_0_sta_0):\n",
    "    training_data_all_ap_0[column] = training_data_all_ap_0[column].apply(process_array_string)\n",
    "\n",
    "training_data_all_ap_1 = training_data_all.loc[training_data_all[\"ap_id\"] == \"ap_1\"].copy()\n",
    "for i, column in enumerate(ap_1_sta_1):\n",
    "    training_data_all_ap_1[ap_0_sta_0[i]] = training_data_all_ap_1[column].apply(process_array_string)\n",
    "\n",
    "training_data_all_processed = pd.concat([training_data_all_ap_0[columns_basic + ap_0_sta_0], training_data_all_ap_1[columns_basic + ap_0_sta_0]], ignore_index=True)\n",
    "\n",
    "\n",
    "######## 训练模型 #######\n",
    "training_data = training_data_all_processed.loc[:, columns_basic + ap_0_sta_0].copy()\n",
    "# 编码非数值变量\n",
    "training_data_encoded = pd.get_dummies(training_data, columns=[\"protocol\"])\n",
    "\n",
    "# 创建新的联合类标签 (nss 和 mcs 组合成一组类)\n",
    "training_data_encoded[\"nss_mcs\"] = training_data_encoded[\"nss\"].astype(str) + \"_\" + training_data_encoded[\"mcs\"].astype(str)\n",
    "\n",
    "training_data_encoded = fill_missing_values(training_data_encoded)\n",
    "\n",
    "# 拼接向量\n",
    "X = training_data_encoded[columns_numerical + ap_0_sta_0 + [col for col in training_data_encoded.columns if col.startswith(\"protocol_\")]]\n",
    "y = training_data_encoded[\"nss_mcs\"]\n",
    "# 移除只有 1 个样本的类别\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_counts = dict(zip(unique, counts))\n",
    "classes_to_keep = [label for label, count in class_counts.items() if count > 1]\n",
    "\n",
    "# 创建掩码，过滤掉样本较少的类别\n",
    "mask = np.isin(y, classes_to_keep)\n",
    "X_filtered = X[mask]\n",
    "y_filtered = y[mask]\n",
    "\n",
    "# 使用 LabelEncoder 将 y 中的字符串标签转换为整数编码\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y_filtered)\n",
    "\n",
    "# 将数据分为训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_filtered, y_encoded, test_size=0.1, stratify=y_encoded, random_state=42)\n",
    "\n",
    "# 标准化特征\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#### 模型训练 ####\n",
    "# 随机森林\n",
    "# model = RandomForestClassifier(random_state=42)\n",
    "# XGBoost\n",
    "# model = xgb.XGBClassifier(objective=\"multi:softmax\", num_class=14, random_state=42)\n",
    "# 线性\n",
    "# model = LogisticRegression(multi_class='multinomial', solver='lbfgs', random_state=42)\n",
    "# 支持向量机\n",
    "# model = SVC(kernel='rbf', random_state=42)\n",
    "# 朴素贝叶斯\n",
    "# model = GaussianNB()\n",
    "model = MLPClassifier(hidden_layer_sizes=(128, 64), max_iter=1000, random_state=42)\n",
    "# AE\n",
    "# model = EncoderDecoderClassifierModel(input_size=X_train_scaled.shape[1], hidden_size=64, epochs=100, lr=0.001, batch_size=1)\n",
    "# VAE\n",
    "# model = VAEClassifierModel(input_size=X_train_scaled.shape[1], hidden_size=64, latent_size=32, epochs=100, lr=0.001, batch_size=1)\n",
    "\n",
    "# 训练模型\n",
    "model.fit(X_train_scaled, y_train)\n",
    "# 对测试集进行预测\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Classification Accuracy: {accuracy}\")\n",
    "\n",
    "# 生成分类报告\n",
    "y_test_labels = label_encoder.inverse_transform(y_test)\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred)\n",
    "unique_labels = sorted(list(set(np.unique(y_test_labels)) | set(np.unique(y_pred_labels))))\n",
    "report = classification_report(y_test_labels, y_pred_labels, labels=unique_labels, target_names=[str(label) for label in unique_labels])\n",
    "print(report)\n",
    "# model.classes_\n",
    "print(y_pred_labels)\n",
    "# y_pred_df = pd.DataFrame([label.split(\"_\") for label in y_pred_labels], columns=[\"nss\", \"mcs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fq/txj212zs79b9zfmsgd_pc3d40000gn/T/ipykernel_50763/3167927537.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[column].fillna(df[column].mean(), inplace=True)\n",
      "/var/folders/fq/txj212zs79b9zfmsgd_pc3d40000gn/T/ipykernel_50763/3167927537.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[column].fillna(0, inplace=True)\n",
      "/var/folders/fq/txj212zs79b9zfmsgd_pc3d40000gn/T/ipykernel_50763/3167927537.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[column].fillna(df[column].mean(), inplace=True)\n",
      "/var/folders/fq/txj212zs79b9zfmsgd_pc3d40000gn/T/ipykernel_50763/3167927537.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[column].fillna(df[column].mean(), inplace=True)\n",
      "/var/folders/fq/txj212zs79b9zfmsgd_pc3d40000gn/T/ipykernel_50763/3167927537.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[column].fillna(df[column].mean(), inplace=True)\n",
      "/var/folders/fq/txj212zs79b9zfmsgd_pc3d40000gn/T/ipykernel_50763/3167927537.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[column].fillna(df[column].mean(), inplace=True)\n",
      "/var/folders/fq/txj212zs79b9zfmsgd_pc3d40000gn/T/ipykernel_50763/3167927537.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[column].fillna(df[column].mean(), inplace=True)\n",
      "/var/folders/fq/txj212zs79b9zfmsgd_pc3d40000gn/T/ipykernel_50763/3167927537.py:28: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[column].fillna(df[column].mean(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "####### 添加了传输方式变换列之后的 test_set 文件夹 。 自动识别 set_2#######\n",
    "\n",
    "test_data_names = get_files_by_keywords(data_dir, [ap_name, \"csv\"])\n",
    "test_data_names = sorted(test_data_names)\n",
    "question2_results_dir = project_root + \"/data/results/question2\"\n",
    "os.makedirs(question2_results_dir, exist_ok=True)\n",
    "for test_data_name in test_data_names:\n",
    "    file_path = os.path.join(data_dir, test_data_name)\n",
    "    test_data_all = pd.read_csv(file_path)\n",
    "\n",
    "    test_data_ap_0 = test_data_all.loc[test_data_all[\"ap_id\"] == \"ap_0\"].copy()\n",
    "    for i, column in enumerate(ap_0_sta_0):\n",
    "        test_data_ap_0[column] = test_data_ap_0[column].apply(process_array_string)\n",
    "\n",
    "    test_data_ap_1 = test_data_all.loc[test_data_all[\"ap_id\"] == \"ap_1\"].copy()\n",
    "    for i, column in enumerate(ap_1_sta_1):\n",
    "        test_data_ap_1[ap_0_sta_0[i]] = test_data_ap_1[column].apply(process_array_string)\n",
    "\n",
    "    test_data_processed = pd.concat([test_data_ap_0[columns_basic + ap_0_sta_0], test_data_ap_1[columns_basic + ap_0_sta_0]], ignore_index=True)\n",
    "\n",
    "    ####### 预测数据 ########\n",
    "    test_data = test_data_processed.loc[:, columns_basic + ap_0_sta_0].copy()\n",
    "    # 编码非数值变量\n",
    "    test_data_encoded = pd.get_dummies(test_data, columns=[\"protocol\"])\n",
    "    X_test_data = test_data_encoded[columns_numerical + ap_0_sta_0 + [col for col in test_data_encoded.columns if col.startswith(\"protocol_\")]]\n",
    "    X_test_data = X_test_data[X_train.columns]\n",
    "\n",
    "    # print(f\"########## {test_data_name} ###########\")\n",
    "    # print(X_test_data.info())\n",
    "    X_test_data = fill_missing_values(X_test_data)\n",
    "\n",
    "    # 对测试数据进行归一化（使用与训练集相同的 scaler）\n",
    "    X_test_final_scaled = scaler.transform(X_test_data)\n",
    "\n",
    "    # 使用训练好的模型进行预测\n",
    "    y_test_pred = model.predict(X_test_final_scaled)\n",
    "    y_test_pred_labels = label_encoder.inverse_transform(y_test_pred)\n",
    "\n",
    "    y_pred_df = pd.DataFrame([label.split(\"_\") for label in y_test_pred_labels], columns=[\"nss\", \"mcs\"])\n",
    "\n",
    "    # 输出预测结果\n",
    "    test_data_all[[\"predict nss\", \"predict mcs\"]] = y_pred_df[[\"nss\", \"mcs\"]]\n",
    "    test_data_all.to_csv(f\"{question2_results_dir}/{test_data_name}\", index=False)\n",
    "\n",
    "    # plt.figure(figsize=(10, 3))\n",
    "    # plt.plot(np.arange(len(test_data[\"predict seq_time\"])), test_data[\"predict seq_time\"], label=\"predict seq_time\")\n",
    "    # plt.plot(np.arange(len(test_data[\"ap_from_ap_1_mean_ant_rssi\"])), test_data[\"ap_from_ap_1_mean_ant_rssi\"], label=\"ap_from_ap_1_mean_ant_rssi\")\n",
    "    # plt.legend()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tangou111",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([\n",
    "    \"ap_from_ap_1_mean_ant_rssi\", \"ap_from_ap_2_mean_ant_rssi\", \n",
    "    \"sta_to_ap_0_mean_ant_rssi\", \"sta_to_ap_1_mean_ant_rssi\", \"sta_to_ap_2_mean_ant_rssi\", \n",
    "    \"sta_from_ap_0_mean_ant_rssi\", \"sta_from_ap_1_mean_ant_rssi\", \"sta_from_ap_2_mean_ant_rssi\", \n",
    "    \"sta_from_sta_1_rssi\", \"sta_from_sta_2_rssi\",\n",
    "    \"ap_from_ap_1_sum_ant_rssi\", \"ap_from_ap_2_sum_ant_rssi\", \n",
    "    \"sta_to_ap_0_sum_ant_rssi\", \"sta_to_ap_1_sum_ant_rssi\", \"sta_to_ap_2_sum_ant_rssi\", \n",
    "    \"sta_from_ap_0_sum_ant_rssi\", \"sta_from_ap_1_sum_ant_rssi\", \"sta_from_ap_2_sum_ant_rssi\", \n",
    "    \"ap_from_ap_1_max_ant_rssi\", \"ap_from_ap_2_max_ant_rssi\", \n",
    "    \"sta_to_ap_0_max_ant_rssi\", \"sta_to_ap_1_max_ant_rssi\", \"sta_to_ap_2_max_ant_rssi\", \n",
    "    \"sta_from_ap_0_max_ant_rssi\", \"sta_from_ap_1_max_ant_rssi\", \"sta_from_ap_2_max_ant_rssi\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daylight/miniconda3/envs/tangou111/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (32) must match the size of tensor b (64) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 151\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;66;03m# 初始化并训练模型\u001b[39;00m\n\u001b[1;32m    150\u001b[0m model \u001b[38;5;241m=\u001b[39m PyTorchTransformerRegressor(model_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m, num_heads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 151\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# 对测试集进行预测\u001b[39;00m\n\u001b[1;32m    154\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_scaled)\n",
      "Cell \u001b[0;32mIn[4], line 98\u001b[0m, in \u001b[0;36mPyTorchTransformerRegressor.fit\u001b[0;34m(self, X, y, X_val, y_val)\u001b[0m\n\u001b[1;32m     96\u001b[0m batch_X, batch_y \u001b[38;5;241m=\u001b[39m batch_X\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), batch_y\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 98\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(outputs, batch_y)\n\u001b[1;32m    100\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/tangou111/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tangou111/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m, in \u001b[0;36mTransformerRegressor.forward\u001b[0;34m(self, src)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# src shape: [batch_size, input_dim]\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_fc(src) \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_dim)  \u001b[38;5;66;03m# [batch_size, model_dim]\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch_size, model_dim]\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     src \u001b[38;5;241m=\u001b[39m src\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Transformer expects [sequence_length, batch_size, model_dim]\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     memory \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_encoder(src)  \u001b[38;5;66;03m# [sequence_length, batch_size, model_dim]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tangou111/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tangou111/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 52\u001b[0m, in \u001b[0;36mPositionalEncoding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# x shape: [batch_size, model_dim]\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpe\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (32) must match the size of tensor b (64) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# 定义Transformer回归模型\n",
    "class TransformerRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim=64, num_heads=4, num_layers=2, dropout=0.1):\n",
    "        super(TransformerRegressor, self).__init__()\n",
    "        self.model_dim = model_dim\n",
    "\n",
    "        self.input_fc = nn.Linear(input_dim, model_dim)\n",
    "        self.pos_encoder = PositionalEncoding(model_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.regressor = nn.Linear(model_dim, 1)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src shape: [batch_size, input_dim]\n",
    "        src = self.input_fc(src) * np.sqrt(self.model_dim)  # [batch_size, model_dim]\n",
    "        src = self.pos_encoder(src)  # [batch_size, model_dim]\n",
    "        src = src.unsqueeze(1)  # Transformer expects [sequence_length, batch_size, model_dim]\n",
    "        memory = self.transformer_encoder(src)  # [sequence_length, batch_size, model_dim]\n",
    "        memory = memory.squeeze(0)  # [batch_size, model_dim]\n",
    "        output = self.regressor(memory)  # [batch_size, 1]\n",
    "        return output.squeeze(1)  # [batch_size]\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, d_model]\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, model_dim]\n",
    "        x = x + self.pe[:, : x.size(1), :]\n",
    "        return x\n",
    "\n",
    "\n",
    "# 封装为类似于XGBRegressor的类\n",
    "class PyTorchTransformerRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, model_dim=64, num_heads=4, num_layers=2, dropout=0.1, lr=1e-3, batch_size=32, epochs=100, device=None, verbose=False):\n",
    "        self.model_dim = model_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.verbose = verbose\n",
    "        self.device = device if device else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = None\n",
    "        self.optimizer = None\n",
    "        self.criterion = None\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        X = torch.tensor(X, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "        if X_val is not None and y_val is not None:\n",
    "            X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "            y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "            val_dataset = TensorDataset(X_val, y_val)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=self.batch_size, shuffle=False)\n",
    "        else:\n",
    "            val_loader = None\n",
    "\n",
    "        dataset = TensorDataset(X, y)\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "        input_dim = X.shape[1]\n",
    "        self.model = TransformerRegressor(input_dim, self.model_dim, self.num_heads, self.num_layers, self.dropout).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            self.model.train()\n",
    "            epoch_loss = 0\n",
    "            for batch_X, batch_y in loader:\n",
    "                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = self.criterion(outputs, batch_y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                epoch_loss += loss.item() * batch_X.size(0)\n",
    "\n",
    "            epoch_loss /= len(loader.dataset)\n",
    "\n",
    "            if self.verbose:\n",
    "                if val_loader:\n",
    "                    val_loss = self.evaluate(val_loader)\n",
    "                    print(f\"Epoch {epoch}/{self.epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}\")\n",
    "                else:\n",
    "                    print(f\"Epoch {epoch}/{self.epochs}, Training Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "        return self\n",
    "\n",
    "    def evaluate(self, loader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in loader:\n",
    "                batch_X, batch_y = batch_X.to(self.device), batch_y.to(self.device)\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = self.criterion(outputs, batch_y)\n",
    "                total_loss += loss.item() * batch_X.size(0)\n",
    "        total_loss /= len(loader.dataset)\n",
    "        return total_loss\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        X = torch.tensor(X, dtype=torch.float32).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X)\n",
    "        return outputs.cpu().numpy()\n",
    "\n",
    "\n",
    "# 示例使用代码\n",
    "if __name__ == \"__main__\":\n",
    "    # 生成一些示例数据\n",
    "    from sklearn.datasets import make_regression\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    random_state_num = 42\n",
    "    X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=random_state_num)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state_num)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # 初始化并训练模型\n",
    "    model = PyTorchTransformerRegressor(model_dim=64, num_heads=4, num_layers=2, dropout=0.1, lr=1e-3, batch_size=32, epochs=100, verbose=True)\n",
    "    model.fit(X_train_scaled, y_train, X_val=X_test_scaled, y_val=y_test)\n",
    "\n",
    "    # 对测试集进行预测\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "    # 对比曲线图：真实值 vs 预测值\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(np.arange(len(y_test)), y_test, label=\"true seq_time\", color=\"blue\", marker=\"o\")\n",
    "    plt.plot(np.arange(len(y_pred)), y_pred, label=\"pred seq_time\", color=\"red\", marker=\"x\")\n",
    "    plt.xlabel(\"Test Samples\")\n",
    "    plt.ylabel(\"seq_time\")\n",
    "    plt.title(\"True vs Predicted seq_time\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tangou111",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
